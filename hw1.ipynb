{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from fake_useragent import UserAgent\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "from string import punctuation\n",
    "\n",
    "ua = UserAgent()\n",
    "headers = {'User-Agent': ua.random}\n",
    "stops = stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parser part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = {}\n",
    "\n",
    "def write_in_dict(url) -> None:\n",
    "    headers_new = {'User-Agent': ua.random}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            page_new = requests.get(url, headers=headers_new)\n",
    "            break\n",
    "        except requests.exceptions.ConnectionError:  # server is disconnecting me for a safety purposes\n",
    "            print(\"Server is unreachable, one more try in 5 seconds.\")  # but we'll try to sneak in\n",
    "            time.sleep(5)\n",
    "\n",
    "    soup_new = bs(page_new.text, \"html.parser\")\n",
    "    for i in soup_new.findAll('div', class_='review_area'):\n",
    "        if i is not None:\n",
    "            text = i.text\n",
    "            rate = re.findall(r\"Score: .*\\s\", text)\n",
    "            rev = re.findall(r\"\\d\\d\\d\\d\\s*(.*)\", text)\n",
    "            if rate:\n",
    "                reviews[rev[0].strip()] = rate[0].strip()\n",
    "\n",
    "\n",
    "def get_films(url, num_pages=0) -> None:\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = bs(page.text, \"html.parser\")\n",
    "\n",
    "    for i in soup.findAll('a'):\n",
    "        link = i.get('href')\n",
    "        if link is not None and link[:2] == '/m':\n",
    "            # print(link)\n",
    "            write_in_dict(f\"https://www.rottentomatoes.com{link}/reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gettin' good rated films\n",
      "Gettin' bad rated films\n"
     ]
    }
   ],
   "source": [
    "print(\"Gettin' good rated films\")\n",
    "get_films(\"https://www.rottentomatoes.com/browse/movies_at_home/audience:upright?page=100\")\n",
    "print(\"Gettin' bad rated films\")\n",
    "get_films('https://www.rottentomatoes.com/browse/movies_at_home/audience:spilled~critics:rotten?page=200')\n",
    "\n",
    "with open('reviews_raw.json', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(reviews, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rate(rate: str) -> int:\n",
    "    \"\"\"\n",
    "    :param rate: raw rate (thing starts with \"Score: ...\")\n",
    "    :return: 0 if rate is bad, 1 if rate is good\n",
    "    \"\"\"\n",
    "\n",
    "    rate = rate.strip(\"Score:\").replace(' ', '').strip('-').strip('+').replace('stars', '').replace('3.5.5', '3.5').replace('of', '/')\n",
    "    try:\n",
    "        letter_rates = {'F': 0,\n",
    "                        'E': 0,\n",
    "                        'D': 0,\n",
    "                        'C': 0,  # could be fine-tuned\n",
    "                        'B': 1,\n",
    "                        'A': 1}\n",
    "\n",
    "        if '/' in rate:  # in case if rating has digits in it\n",
    "            rate = int(round(float(rate.split('/')[0]) / float(rate.split('/')[1])))\n",
    "            if rate > 1:\n",
    "                return 1\n",
    "            return rate\n",
    "        elif rate.upper() in letter_rates:  # in case if rating letter-based\n",
    "            return letter_rates[rate.upper()]\n",
    "        else:\n",
    "            return int(round(float(rate) / 10.))\n",
    "    except ValueError:\n",
    "        print(rate) # bruh, completely weird rating\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence: str) -> str:\n",
    "    # sent = ' '.join(nltk.word_tokenize(sentence))\n",
    "    doc = nlp(sentence)\n",
    "    sent = [token.lemma_.lower() for token in doc]\n",
    "    sent = ' '.join([w for w in sent if w not in punctuation and w not in stops])\n",
    "    sent = pos_filter(sent, allowed=['ADJ', 'NOUN', 'VERB'])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_filter(sentence: str, allowed: list) -> str:\n",
    "    doc = nlp(sentence)\n",
    "    return ' '.join([token.text for token in doc if token.pos_ in allowed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(text) -> Counter:\n",
    "    tf_text = Counter(text)\n",
    "    for i in tf_text:\n",
    "        tf_text[i] = tf_text[i]/float(len(text))\n",
    "    return tf_text\n",
    "\n",
    "def compute_idf(word, corpus) -> float:\n",
    "    return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "def tf_idf(corpus) -> list:\n",
    "    documents = []    \n",
    "    for text in corpus:\n",
    "        tf_idf_words = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "\n",
    "        for word in computed_tf:\n",
    "            tf_idf_words[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        \n",
    "        documents.append(Counter(tf_idf_words).most_common(5))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actual working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-plus\n",
      "B-plus\n",
      "BigScreenWatch\n",
      "BigScreenWatch\n",
      "ReadABook\n",
      "ReadABook\n",
      "Reviews: 4504\n",
      "Good rates: 2768, bad rates: 1736\n"
     ]
    }
   ],
   "source": [
    "with open('reviews_raw.json', 'r', encoding=\"utf-8\") as f,\\\n",
    "        open('reviews_normalized.json', 'w', encoding=\"utf-8\") as n:\n",
    "    reviews = json.load(f)\n",
    "    goods = 0\n",
    "    new_revs = {}\n",
    "    for r in reviews:\n",
    "        old_rate = reviews[r]\n",
    "        lemm_sent = lemmatize_sentence(r)\n",
    "        goods += normalize_rate(old_rate)\n",
    "        new_revs[lemm_sent] = normalize_rate(old_rate)\n",
    "\n",
    "    n.write(json.dumps(new_revs, ensure_ascii=False, indent=4))\n",
    "\n",
    "    print(f\"Reviews: {len(new_revs)}\")\n",
    "    print(f\"Good rates: {goods}, bad rates: {len(new_revs) - goods}\")\n",
    "\n",
    "    bad_rates = []\n",
    "    good_rates = []\n",
    "\n",
    "    for i in new_revs:\n",
    "        if new_revs[i] == 0:\n",
    "            bad_rates.append(nltk.word_tokenize(i))\n",
    "        else:\n",
    "            good_rates.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = set()\n",
    "for sentence in tf_idf(bad_rates):\n",
    "    for word in sentence:\n",
    "        bad_words.add(str(word[0]))\n",
    "\n",
    "good_words = set()\n",
    "for sentence in tf_idf(good_rates):\n",
    "    for word in sentence:\n",
    "        good_words.add(str(word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем слова находящиеся только в одном списке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bad_words.difference(good_words)\n",
    "g = good_words.difference(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentence(sent: str) -> str:\n",
    "    sent = lemmatize_sentence(sent)\n",
    "    sent = set(nltk.word_tokenize(sent))\n",
    "    if len(sent.intersection(g)) > len(sent.intersection(b)):  # just check words from which set are more often\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = [\n",
    "    (\"I thought this film brought a new concept to screen, wild turns and dark themed. Loved it.\", \"Positive\"),\n",
    "    (\"My god this movie is so bad and boring and people say this movie is great your wrong is a terrible action movie and it totally rip off 1999 action hit film The Matrix.\", \"Negative\"),\n",
    "    (\"I thought torture was outlawed by the Geneva Convention in 1949 but Ballistic: Ecks vs. Sever proves that atrocities like these are far from over.\", \"Negative\"),\n",
    "    (\"What do you mean? I loved it!\", \"Positive\"),\n",
    "    (\"Very dissapointing held together by a decent cast.\", \"Negative\"),\n",
    "    (\"The entire movie is dumb. The long, boring, unnecessary intro is big foreshadowing of what's to come: more boring, unnecessarily long scenes filled with a whole lot of nothingness. The end is dumb, too.\", \"Negative\"),\n",
    "    (\"Windfall is being touted as a pedestrian thriller when the attention rightfully belongs to its astute satire and triangular approach to social perspective.\", \"Positive\")\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Negative  Correct: Positive\n",
      "Prediction: Negative  Correct: Negative\n",
      "Prediction: Positive  Correct: Negative\n",
      "Prediction: Negative  Correct: Positive\n",
      "Prediction: Negative  Correct: Negative\n",
      "Prediction: Negative  Correct: Negative\n",
      "Prediction: Positive  Correct: Positive\n"
     ]
    }
   ],
   "source": [
    "for i in test_batch:\n",
    "    print(f\"Prediction: {check_sentence(i[0])}  Correct: {i[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты не очень впечатляют. Мои предложения по улучшению:\n",
    "1. Возможно есть корреляция положительности/отрицательности отзыва и длинны этого отзыва (плохие фильмы больше ругают)\n",
    "2. Искать биграммы вроде \"terrible movie\" или \"great work\" и проверять есть ли отрицание этой биграммы\n",
    "3. Построить вектора для каждого слова и сравнивать общий вектор предложения с каким-то \"общим негативным/позитивным\" вектором"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "364dd72bdfeade7174b7ce65c69812238e898dab5473323adf3eae7e642533d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
